{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vježba 3 - Vektorska reprezentacija dokumenta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol><li> Kreirati promjenljivu <b>str1</b> koja sadrži tekst 'zdravo', te listu <b>str_list</b> koja sadrži stringove: 'zdravo', 'zbogom', 'zdravo', 'zbogom', 'zdravo', 'zdravoratumski'. Napisati kod koji broji koliko puta se string 'zdravo' nalazi u listi <b>str_list</b>.\n",
    "</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rijec zdravo se pojavljuje 4 puta u listi str_list\n"
     ]
    }
   ],
   "source": [
    "#KOD ZA ZADATAK 1\n",
    "str1 = 'zdravo'\n",
    "str_list = ['zdravo', 'zbogom', 'zdravo', 'zbogom', 'zdravo', 'zdravoratumski']\n",
    "counter = 0\n",
    "for str_entity in str_list:\n",
    "    if str1 in str_entity:\n",
    "        counter += 1\n",
    "print('Rijec', str1, 'se pojavljuje', counter, 'puta u listi str_list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol start = \"2\">\n",
    "<li> Napisati kod koji se u listi <b>str_list</b> pronalazi string najsličniji riječi 'zdravorazumski', pri čemu se sličnost mjeri brojem istih slova.\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Najveci moguci broj je 9223372036854775807\n",
      "Najslicniji zdravorazumski je zdravoratumski\n",
      "Rijec zdravo ima 6 istih slova kao i rijec zdravorazumski\n",
      "Rijec zbogom ima 4 istih slova kao i rijec zdravorazumski\n",
      "Rijec zdravo ima 6 istih slova kao i rijec zdravorazumski\n",
      "Rijec zbogom ima 4 istih slova kao i rijec zdravorazumski\n",
      "Rijec zdravo ima 6 istih slova kao i rijec zdravorazumski\n",
      "Rijec zdravoratumski ima 13 istih slova kao i rijec zdravorazumski\n"
     ]
    }
   ],
   "source": [
    "#KOD ZA ZADATAK 2\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "word_to_find = 'zdravorazumski'\n",
    "\n",
    "def getOccurance(text):\n",
    "    lower_case = np.zeros(26)\n",
    "    for character in text:\n",
    "        lower_case[ord(character) - ord('a')] += 1\n",
    "    return lower_case\n",
    "\n",
    "zdrm = getOccurance(word_to_find)\n",
    "\n",
    "index_of_most_similar = -1\n",
    "similarity_value = sys.maxsize\n",
    "print('Najveci moguci broj je', sys.maxsize)\n",
    "\n",
    "for i, string in enumerate(str_list):\n",
    "    occurances = getOccurance(string)\n",
    "    temp_similarity = np.abs(occurances - zdrm).sum()\n",
    "    if temp_similarity < similarity_value:\n",
    "        similarity_value = temp_similarity\n",
    "        index_of_most_similar = i\n",
    "print('Najslicniji', word_to_find, 'je', str_list[index_of_most_similar])\n",
    "\n",
    "\n",
    "slicna_rijec = 'zdravorazumski'\n",
    "counters = [0, 0, 0, 0, 0, 0]\n",
    "counter = 0\n",
    "for rijec in str_list:\n",
    "    for i in range(len(rijec)):\n",
    "        if rijec[i] in slicna_rijec:\n",
    "            counters[counter] += 1\n",
    "    print('Rijec', rijec, 'ima', counters[counter], 'istih slova kao i rijec', slicna_rijec)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol start = \"3\">\n",
    "<li> Kreirajte promjenljivu koja sadrži string 'eci peci pec ti si mali zec'. Tokenizujte ovu promjenljivu na riječi koje sadrži. Rezultat treba da bude lista koja sadrži pojedine riječi zadatog stringa.\n",
    "</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> \n",
      " ['eci', 'peci', 'pec', 'ti', 'si', 'mali', 'zec']\n",
      "<class 'set'> \n",
      " {'si', 'pec', 'zec', 'eci', 'mali', 'peci', 'ti'}\n"
     ]
    }
   ],
   "source": [
    "#KOD ZA ZADATAK 3\n",
    "sentence = 'eci peci pec ti si mali zec'\n",
    "token = sentence.split()\n",
    "print(type(token), '\\n', token)\n",
    "\n",
    "rabbit_tokens = { x for x in sentence.split() }\n",
    "print(type(rabbit_tokens), '\\n', rabbit_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "U narednom dijelu vježbe ćemo iskoristiti korpus dokumenata sa projekta Gutenberg da bismo kreirali njihovu vektorsku reprezentaciju, a nakon toga implementirali sistem koji za proizvoljan dokument pronalazi najsličniji dokument iz Gutenberg korpusa. U narednoj ćeliji koda su date dvije funkcije koje se mogu iskoristiti za tokenizaciju dokumenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "minlen = 1\n",
    "\n",
    "def normalize(word):\n",
    "    # Funkcija koja vrši normalizaciju riječi\n",
    "    word = re.sub('[^A-Za-z0-9]+', '', word)    #Ukljanjanje svih karakter koji nisu alfanumerički\n",
    "    word = re.sub('[0-9]+', 'number', word)     #Zamjena svih brojeva u tekstu riječju number\n",
    "    word = word.lower()                         #Pretvaranje u mala slova\n",
    "    return word\n",
    "\n",
    "def tokenize(text):\n",
    "    #Funkcija koja vrši tokenizaciju teksta na sastavne riječi\n",
    "    tokens = nltk.word_tokenize(text)           #Tokenizacija teksta\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        word = normalize(token)                 #Normalizacija pronađenih tokena\n",
    "        stem = stemmer.stem(word)               #Stemizacija tokena\n",
    "        if len(stem) > minlen:                  #Zadržavanje tokena čija je dužina veća od minlen\n",
    "            stems.append(stem)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potrebno je napisati kod koji će za sve dokumente iz Gutenberg korpusa izračunati njihovu vektorsku reprezentaciju i to sačuvati u promjenljivu representations. Da bi se to uradilo potrebno je:\n",
    "1. Za svaki dokument odrediti tokene,\n",
    "2. Odrediti rječnik za dati korpus,\n",
    "3. Za svaki dokument na osnovu rječnika odrediti vektorsku reprezentaciju.\n",
    "U narednoj ćeliji su date kostur potrebnog koda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "29670\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "#Putanja do korpusa\n",
    "path = './gutenberg'\n",
    "\n",
    "#Dictionary koji će za svaki dokument sadržati njegove tokene\n",
    "token_dict = {}\n",
    "\n",
    "#Prolazak kroz sve fajlove\n",
    "for dirpath, dirs, files in os.walk(path):\n",
    "    for f in files:\n",
    "        fname = os.path.join(dirpath, f)    #Putanja do fajla\n",
    "        #Izršiti učitavanje fajla i njegovu tokenizaciju\n",
    "        #Tokene svakog fajla upisati u token_dict i to tako da je ključ naziv fajla, a vrijednost tokeni\n",
    "        #KOD\n",
    "        with open(fname, 'r') as file:\n",
    "            text = file.read()\n",
    "            tokens = tokenize(text)\n",
    "            token_dict[fname] = tokens\n",
    "\n",
    "#Lista koja će predstavljati rječnik svih rječi iz korpusa\n",
    "dictionary = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#Proći kroz tokene za sve fajlove i dodati ih u dictionary, obratiti pažnju da često korištene riječi (stop_words) nije potrebno\n",
    "#dodavati u rječnik\n",
    "\n",
    "#KOD\n",
    "\n",
    "for tokens in token_dict.values():\n",
    "    for token in tokens:\n",
    "        if not token in dictionary and not token in stop_words:\n",
    "            dictionary.append(token)\n",
    "            \n",
    "#Kako rječnik ne bi trebalo da sadrži ponovljene riječi potrebno je ostaviti samo jedinstvene riječi\n",
    "\n",
    "#KOD\n",
    "\n",
    "# odradjeno u if uslovu gore\n",
    "\n",
    "#Kako bi se ubrzalo pretraživanje rječnika, rječnik se pretvara u dictionary koji za ključeve ima riječi, a za vrijednosti\n",
    "#indeks koji odgovara poziciji te riječi u reprezentaciji\n",
    "\n",
    "dictionary_dict = {word: i for i, word in enumerate(dictionary)}\n",
    "\n",
    "print(len(dictionary_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_document_representation(tokens):\n",
    "    #Napisati funkciju koja kreira reprezentaciju dokumenta na osnovu ulaznih tokena i to tako da se za svaku riječ iz rječnika\n",
    "    #odredi koliko puta se pojavljuje u dokumentu\n",
    "    #Izlaz treba biti niz ranga 1 dužine koja odgovara dužini rječnika\n",
    "    #KOD\n",
    "    #pass\n",
    "    vector = np.zeors(len(dictionary_dict))\n",
    "    for token in tokens:\n",
    "        if token in dictionary_dict:\n",
    "            vector[dictionary_dict[token]] += 1\n",
    "    return vector\n",
    "    \n",
    "#Kreiranje numpy niza koji će sadržati reprezentacije svakog dokumenta iz korpusa, shape je (broj_dokumenata, broj_rječi)\n",
    "representations = np.zeros((len(token_dict), len(dictionary_dict)))\n",
    "\n",
    "#Kreirati reprezentaciju svih dokumenta iz korpusa tako što ćete iskoristiti prethodno napisanu funkciju\n",
    "\n",
    "#KOD\n",
    "for i, tokens in enumerate(token_dict.values()):\n",
    "    representations[i] = compute_document_representation(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nakon što smo odredili reprezentaciju za sve dokumente ove reprezentacije je moguće iskoristiti za pretraživanje. Pretraživanje izvršiti na način da se za proizvoljno izabran dokument pronalaze tri najsličnija dokumenta iz korpusa. Za računanje sličnosti iskoristiti i SSD i kosinusnu sličnost. Za računanje kosinusne sličnosti je moguće iskoristiti funkciju cosine_similarity iz modula sklearn.metrics.pairwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#KOD\n",
    "doc_id = 0\n",
    "\n",
    "sims = cosine_similarity(representations[doc_id].reshape(1, -1), representations)\n",
    "ssd = { ((a - representations[doc_id]) ** 2).sum(): i for i, a in enumerate(representations) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za računanje TFIDF reprezentacije dokumenta moguće je iskoristiti sklearn biblioteku i kod dat u narednoj ćeliji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "#Putanja do korpusa\n",
    "path = './gutenberg'\n",
    "\n",
    "#Dictionary koji sadrži tekst svakog dokumenta\n",
    "text_dict = {}\n",
    "\n",
    "#Čitanje sadržaja svih dokumenata\n",
    "for dirpath, dirs, files in os.walk(path):\n",
    "    for f in files:\n",
    "        fname = os.path.join(dirpath, f)\n",
    "        print (fname)\n",
    "        with open(fname) as file:\n",
    "            text_dict[f] = file.read()\n",
    "\n",
    "#Definisanje objekta koji služi za određivanje TFIDF reprezentacije, za tokenizaciju se koristifunkcije data\n",
    "#kao parametar tokenize, dok se kao stop riječi koriste one iz enegleskog jezika (moguće je zadati i listu koja sadrži stop riječi)\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "\n",
    "#Učenje rječnika i transformacija dokumenata koji su iskorišteni za učenje rječnika\n",
    "#U representations_tfidf se sada nalazi rezrezentacija svih dokumenata iz korpusa\n",
    "representations_tfidf = tfidf.fit_transform(text_dict.values())\n",
    "\n",
    "str = 'all great and precious things are lonely.'\n",
    "\n",
    "#Određivanje reprezentacije za novi dokument\n",
    "#Treba obratiti pažnju da su reprezentacije date kao sparse matrice radi efikasnijg čuvanja u memoriji ali i bržeg procesiranja.\n",
    "response = tfidf.transform([str])\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za proizvoljan dokument odrediti 3 najsličnija dokumenta iz korpusa na osnovu SSD i kosinusne sličnosti. Prethodno navedenu funkciju za kosinusnu sličnost je moguće koristiti i sa sparse matricama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KOD\n",
    "\n",
    "sort_args_sims = sims.argsort()[0][::-1]\n",
    "sort_args_ssd = sorted(ssd)\n",
    "\n",
    "for i in sort_args_sims[:3:1]:\n",
    "    print(list(token_dict)[1])\n",
    "    \n",
    "print()\n",
    "\n",
    "for i in sort_args_ssd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
